{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3474471b",
   "metadata": {},
   "outputs": [],
   "source": "# Transfer Learning: Multi-GNN for Node-Level Sanctions Classification\n\nAdapting the [IBM Multi-GNN](https://github.com/IBM/Multi-GNN) architecture (originally for **edge-level** AML detection) to perform **node-level** classification of sanctioned entities on blockchain transaction graphs.\n\n**Key Architectural Changes:**\n- **Edge-level → Node-level** prediction\n- **CSV → Parquet** data format\n- **Final MLP**: `3×hidden` → `1×hidden` (no src+dst+edge concatenation — use node embeddings directly)\n- **Class-weighted loss** for severe imbalance (~82 sanctioned vs ~19,000 non-sanctioned)"
  },
  {
   "cell_type": "code",
   "id": "93n5rfv4wz4",
   "source": "# Step 1-2: Install required dependencies\n!pip install torch torch-geometric pandas numpy scikit-learn matplotlib seaborn tqdm pyarrow",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "u926omah78f",
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINEConv, GATConv, BatchNorm, Linear\nfrom torch_geometric.data import Data\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\nfrom sklearn.metrics import (\n    f1_score, precision_score, recall_score, accuracy_score,\n    confusion_matrix, classification_report\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "t9evb41cmz",
   "source": "## 1. Data Loading\n\nLoading three data files:\n- `formatted_transactions.parquet` — edge list with transaction features\n- `node_labels.parquet` — node-level labels (sanctioned vs non-sanctioned)\n- `data_splits.json` — train/validation split by edge IDs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "49lajq6151p",
   "source": "# Step 3: Upload data files (Colab) or set local paths\n\n# --- For Google Colab: uncomment these lines to upload files ---\n# from google.colab import files\n# uploaded = files.upload()\n\n# --- Set data directory ---\nDATA_DIR = \".\"  # adjust if your files are in a subfolder\n\ntransactions_path = os.path.join(DATA_DIR, \"formatted_transactions.parquet\")\nlabels_path = os.path.join(DATA_DIR, \"node_labels.parquet\")\nsplits_path = os.path.join(DATA_DIR, \"data_splits.json\")\n\n# Load data\ndf_edges = pd.read_parquet(transactions_path)\ndf_labels = pd.read_parquet(labels_path)\nwith open(splits_path, 'r') as f:\n    data_splits = json.load(f)\n\nprint(\"=== Transactions ===\")\nprint(f\"Shape: {df_edges.shape}\")\nprint(f\"Columns: {df_edges.columns.tolist()}\")\nprint(df_edges.head())\n\nprint(\"\\n=== Node Labels ===\")\nprint(f\"Shape: {df_labels.shape}\")\nprint(f\"Columns: {df_labels.columns.tolist()}\")\nprint(df_labels.head())\nprint(f\"\\nLabel distribution:\\n{df_labels.iloc[:, -1].value_counts()}\")\n\nprint(\"\\n=== Data Splits ===\")\nprint(f\"Keys: {list(data_splits.keys())}\")\nfor k, v in data_splits.items():\n    if isinstance(v, list):\n        print(f\"  {k}: {len(v)} entries (first 5: {v[:5]})\")\n    else:\n        print(f\"  {k}: {v}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h0ls4unnwzm",
   "source": "# Step 4-5: Configure column names based on the output above\n# *** ADJUST THESE IF YOUR COLUMNS HAVE DIFFERENT NAMES ***\n\nSRC_COL = \"from_id\"       # Source node column\nDST_COL = \"to_id\"         # Destination node column\n\n# Edge feature columns = everything except src/dst IDs\nEXCLUDE_COLS = {SRC_COL, DST_COL}\nEDGE_FEATURE_COLS = [c for c in df_edges.columns if c not in EXCLUDE_COLS]\nprint(f\"Edge feature columns ({len(EDGE_FEATURE_COLS)}): {EDGE_FEATURE_COLS}\")\n\n# Node label columns (assumes first col = node ID, last col = label)\nNODE_ID_COL = df_labels.columns[0]\nLABEL_COL = df_labels.columns[-1]\nprint(f\"Node ID column: {NODE_ID_COL}\")\nprint(f\"Label column: {LABEL_COL}\")\nprint(f\"\\nSplit keys available: {list(data_splits.keys())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xt0iqw50j4",
   "source": "# Step 6-8: Build the PyTorch Geometric Data object\n\ndef z_norm(data):\n    \"\"\"Z-score normalization (same as original Multi-GNN).\"\"\"\n    std = data.std(0).unsqueeze(0)\n    std = torch.where(std == 0, torch.tensor(1.0), std)\n    return (data - data.mean(0).unsqueeze(0)) / std\n\n\ndef build_pyg_data(df_edges, df_labels, data_splits,\n                   src_col, dst_col, edge_feature_cols,\n                   node_id_col, label_col):\n    \"\"\"\n    Build a PyG Data object for node-level classification.\n\n    Original IBM Multi-GNN builds an edge-level graph with per-edge labels.\n    Here we keep the same graph structure but attach per-node labels and masks.\n    \"\"\"\n    # --- Edge index [2, num_edges] ---\n    src = torch.LongTensor(df_edges[src_col].values)\n    dst = torch.LongTensor(df_edges[dst_col].values)\n    edge_index = torch.stack([src, dst], dim=0)\n\n    # --- Edge attributes ---\n    edge_feat_df = df_edges[edge_feature_cols].copy()\n    for col in edge_feat_df.columns:\n        if edge_feat_df[col].dtype == object:\n            edge_feat_df[col] = edge_feat_df[col].astype('category').cat.codes\n    edge_attr = torch.tensor(edge_feat_df.values, dtype=torch.float32)\n\n    # --- Node features: placeholder of all 1s (same as original) ---\n    max_node_id = max(int(src.max()), int(dst.max())) + 1\n    x = torch.ones(max_node_id, 1, dtype=torch.float32)\n\n    # --- Node labels ---\n    y = torch.zeros(max_node_id, dtype=torch.long)\n    label_node_ids = df_labels[node_id_col].values\n    label_values = df_labels[label_col].values\n    for nid, lbl in zip(label_node_ids, label_values):\n        if nid < max_node_id:\n            y[int(nid)] = int(lbl)\n\n    # --- Train/Val node masks derived from edge-based splits ---\n    train_mask = torch.zeros(max_node_id, dtype=torch.bool)\n    val_mask = torch.zeros(max_node_id, dtype=torch.bool)\n\n    # Find train/val keys in data_splits\n    train_key = val_key = None\n    for k in data_splits.keys():\n        kl = k.lower()\n        if 'train' in kl:\n            train_key = k\n        elif 'val' in kl or 'valid' in kl:\n            val_key = k\n\n    if train_key and isinstance(data_splits[train_key], list):\n        train_edge_ids = data_splits[train_key]\n        train_edges_df = df_edges.iloc[train_edge_ids]\n        train_nodes = set(train_edges_df[src_col].values) | set(train_edges_df[dst_col].values)\n        for nid in train_nodes:\n            if nid < max_node_id:\n                train_mask[int(nid)] = True\n\n    if val_key and isinstance(data_splits[val_key], list):\n        val_edge_ids = data_splits[val_key]\n        val_edges_df = df_edges.iloc[val_edge_ids]\n        val_nodes = set(val_edges_df[src_col].values) | set(val_edges_df[dst_col].values)\n        for nid in val_nodes:\n            if nid < max_node_id:\n                val_mask[int(nid)] = True\n        # Nodes in both splits stay in train only\n        val_mask = val_mask & ~train_mask\n\n    if train_key is None or val_key is None:\n        print(\"WARNING: Could not auto-detect train/val keys in data_splits.json.\")\n        print(f\"  Available keys: {list(data_splits.keys())}\")\n        print(\"  Falling back to random 80/20 split on all nodes.\")\n        perm = torch.randperm(max_node_id)\n        split_idx = int(0.8 * max_node_id)\n        train_mask[perm[:split_idx]] = True\n        val_mask[perm[split_idx:]] = True\n\n    # --- Normalize features ---\n    edge_attr = z_norm(edge_attr)\n    x = z_norm(x)\n\n    data = Data(\n        x=x,\n        edge_index=edge_index,\n        edge_attr=edge_attr,\n        y=y,\n        train_mask=train_mask,\n        val_mask=val_mask,\n    )\n    return data\n\n\n# Build the data object\ndata = build_pyg_data(\n    df_edges, df_labels, data_splits,\n    SRC_COL, DST_COL, EDGE_FEATURE_COLS,\n    NODE_ID_COL, LABEL_COL\n)\n\nprint(\"Graph Data Object:\")\nprint(f\"  Nodes:           {data.num_nodes:,}\")\nprint(f\"  Edges:           {data.num_edges:,}\")\nprint(f\"  Node features:   {data.x.shape}\")\nprint(f\"  Edge features:   {data.edge_attr.shape}\")\nprint(f\"  Labels:          {data.y.shape}\")\nprint(f\"  Train nodes:     {data.train_mask.sum().item():,}\")\nprint(f\"  Val nodes:       {data.val_mask.sum().item():,}\")\nprint(f\"  Sanctioned (y=1):{(data.y == 1).sum().item():,}\")\nprint(f\"  Non-sanctioned:  {(data.y == 0).sum().item():,}\")\nprint(f\"  Train sanctioned:{(data.y[data.train_mask] == 1).sum().item()}\")\nprint(f\"  Val sanctioned:  {(data.y[data.val_mask] == 1).sum().item()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8brlcom6rel",
   "source": "## 2. Model Architecture\n\nAdapting the IBM Multi-GNN models (GINe / GATe) from **edge-level** to **node-level** prediction.\n\n| | Original (edge-level) | Adapted (node-level) |\n|---|---|---|\n| **After GNN layers** | Concatenate `[src ‖ dst ‖ edge]` | Use node embeddings directly |\n| **MLP input dim** | `3 × n_hidden` | `1 × n_hidden` |\n| **Output** | One prediction per edge | One prediction per node |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kaa8xwu1y8",
   "source": "# Step 9-12: Adapted GNN models for node-level classification\n\nclass NodeGINe(nn.Module):\n    \"\"\"\n    GIN with edge features (GINe), adapted for node-level classification.\n\n    Original IBM Multi-GNN outputs edge predictions by concatenating\n    src + dst + edge embeddings (3*n_hidden -> MLP).\n    This version outputs node predictions directly from node embeddings\n    after message passing (n_hidden -> MLP).\n    \"\"\"\n    def __init__(self, num_features, num_gnn_layers, n_classes=2,\n                 n_hidden=100, edge_updates=False, residual=True,\n                 edge_dim=None, dropout=0.0, final_dropout=0.5):\n        super().__init__()\n        self.n_hidden = n_hidden\n        self.num_gnn_layers = num_gnn_layers\n        self.edge_updates = edge_updates\n        self.final_dropout = final_dropout\n\n        self.node_emb = nn.Linear(num_features, n_hidden)\n        self.edge_emb = nn.Linear(edge_dim, n_hidden)\n\n        self.convs = nn.ModuleList()\n        self.emlps = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        for _ in range(self.num_gnn_layers):\n            conv = GINEConv(nn.Sequential(\n                nn.Linear(n_hidden, n_hidden),\n                nn.ReLU(),\n                nn.Linear(n_hidden, n_hidden)\n            ), edge_dim=n_hidden)\n            if self.edge_updates:\n                self.emlps.append(nn.Sequential(\n                    nn.Linear(3 * n_hidden, n_hidden),\n                    nn.ReLU(),\n                    nn.Linear(n_hidden, n_hidden),\n                ))\n            self.convs.append(conv)\n            self.batch_norms.append(BatchNorm(n_hidden))\n\n        # CHANGED: MLP input is n_hidden (node embedding only)\n        # Original was 3*n_hidden (src_emb + dst_emb + edge_emb)\n        self.mlp = nn.Sequential(\n            Linear(n_hidden, 50), nn.ReLU(), nn.Dropout(self.final_dropout),\n            Linear(50, 25), nn.ReLU(), nn.Dropout(self.final_dropout),\n            Linear(25, n_classes)\n        )\n\n    def forward(self, x, edge_index, edge_attr):\n        src, dst = edge_index\n\n        x = self.node_emb(x)\n        edge_attr = self.edge_emb(edge_attr)\n\n        for i in range(self.num_gnn_layers):\n            x = (x + F.relu(self.batch_norms[i](self.convs[i](x, edge_index, edge_attr)))) / 2\n            if self.edge_updates:\n                edge_attr = edge_attr + self.emlps[i](\n                    torch.cat([x[src], x[dst], edge_attr], dim=-1)\n                ) / 2\n\n        # CHANGED: Return node-level predictions directly\n        # Original did: x[edge_index.T].reshape(-1, 2*n_hidden) then cat with edge_attr\n        return self.mlp(x)\n\n\nclass NodeGATe(nn.Module):\n    \"\"\"\n    GAT with edge features (GATe), adapted for node-level classification.\n    Same structural change as NodeGINe: node embeddings -> MLP instead of\n    edge readout -> MLP.\n    \"\"\"\n    def __init__(self, num_features, num_gnn_layers, n_classes=2,\n                 n_hidden=100, n_heads=4, edge_updates=False,\n                 edge_dim=None, dropout=0.0, final_dropout=0.5):\n        super().__init__()\n        tmp_out = n_hidden // n_heads\n        n_hidden = tmp_out * n_heads\n\n        self.n_hidden = n_hidden\n        self.n_heads = n_heads\n        self.num_gnn_layers = num_gnn_layers\n        self.edge_updates = edge_updates\n        self.dropout = dropout\n        self.final_dropout = final_dropout\n\n        self.node_emb = nn.Linear(num_features, n_hidden)\n        self.edge_emb = nn.Linear(edge_dim, n_hidden)\n\n        self.convs = nn.ModuleList()\n        self.emlps = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        for _ in range(self.num_gnn_layers):\n            conv = GATConv(n_hidden, tmp_out, n_heads, concat=True,\n                           dropout=dropout, add_self_loops=True, edge_dim=n_hidden)\n            if self.edge_updates:\n                self.emlps.append(nn.Sequential(\n                    nn.Linear(3 * n_hidden, n_hidden),\n                    nn.ReLU(),\n                    nn.Linear(n_hidden, n_hidden),\n                ))\n            self.convs.append(conv)\n            self.batch_norms.append(BatchNorm(n_hidden))\n\n        # CHANGED: MLP input is n_hidden (node embedding only)\n        self.mlp = nn.Sequential(\n            Linear(n_hidden, 50), nn.ReLU(), nn.Dropout(self.final_dropout),\n            Linear(50, 25), nn.ReLU(), nn.Dropout(self.final_dropout),\n            Linear(25, n_classes)\n        )\n\n    def forward(self, x, edge_index, edge_attr):\n        src, dst = edge_index\n\n        x = self.node_emb(x)\n        edge_attr = self.edge_emb(edge_attr)\n\n        for i in range(self.num_gnn_layers):\n            x = (x + F.relu(self.batch_norms[i](self.convs[i](x, edge_index, edge_attr)))) / 2\n            if self.edge_updates:\n                edge_attr = edge_attr + self.emlps[i](\n                    torch.cat([x[src], x[dst], edge_attr], dim=-1)\n                ) / 2\n\n        # CHANGED: Return node-level predictions directly\n        return self.mlp(x)\n\n\nprint(\"NodeGINe and NodeGATe models defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qf1mplfiuef",
   "source": "## 3. Training\n\n- **Loss**: Weighted `CrossEntropyLoss` — automatically computes class weight from the train set imbalance ratio\n- **Metrics**: Loss, Accuracy, Precision, Recall, F1 (tracked per epoch for train and val)\n- **Checkpointing**: Best model saved based on validation F1 score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n61hi6r9aa",
   "source": "# Step 13-17: Training with class-weighted loss, metrics tracking, and checkpointing\n\ndef compute_metrics(pred, target):\n    \"\"\"Compute classification metrics.\"\"\"\n    pred_np = pred.cpu().numpy()\n    target_np = target.cpu().numpy()\n    return {\n        'accuracy': accuracy_score(target_np, pred_np),\n        'precision': precision_score(target_np, pred_np, zero_division=0),\n        'recall': recall_score(target_np, pred_np, zero_division=0),\n        'f1': f1_score(target_np, pred_np, zero_division=0),\n    }\n\n\ndef train_epoch(model, data, optimizer, loss_fn):\n    \"\"\"Train for one epoch on the full graph (transductive).\"\"\"\n    model.train()\n    optimizer.zero_grad()\n\n    out = model(data.x, data.edge_index, data.edge_attr)\n    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n\n    loss.backward()\n    optimizer.step()\n\n    pred = out[data.train_mask].argmax(dim=-1)\n    metrics = compute_metrics(pred, data.y[data.train_mask])\n    metrics['loss'] = loss.item()\n    return metrics\n\n\n@torch.no_grad()\ndef evaluate(model, data, loss_fn, mask):\n    \"\"\"Evaluate on a subset of nodes defined by mask.\"\"\"\n    model.eval()\n\n    out = model(data.x, data.edge_index, data.edge_attr)\n    loss = loss_fn(out[mask], data.y[mask])\n\n    pred = out[mask].argmax(dim=-1)\n    metrics = compute_metrics(pred, data.y[mask])\n    metrics['loss'] = loss.item()\n    return metrics, pred, out[mask]\n\n\ndef train(model, data, n_epochs=100, lr=0.006, save_path=\"best_model.pt\"):\n    \"\"\"Full training loop with validation and checkpointing.\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # Step 13: Class-weighted loss — weight the minority class by the imbalance ratio\n    n_sanctioned = (data.y[data.train_mask] == 1).sum().float()\n    n_non_sanctioned = (data.y[data.train_mask] == 0).sum().float()\n    weight_ratio = (n_non_sanctioned / n_sanctioned).item() if n_sanctioned > 0 else 1.0\n\n    class_weights = torch.FloatTensor([1.0, weight_ratio]).to(device)\n    print(f\"Class weights: [non-sanctioned: {class_weights[0]:.2f}, sanctioned: {class_weights[1]:.2f}]\")\n    print(f\"  (ratio = {weight_ratio:.1f}:1)\\n\")\n\n    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\n    # Tracking\n    history = {\n        'train_loss': [], 'train_acc': [], 'train_prec': [],\n        'train_rec': [], 'train_f1': [],\n        'val_loss': [], 'val_acc': [], 'val_prec': [],\n        'val_rec': [], 'val_f1': [],\n    }\n    best_val_f1 = 0.0\n    best_epoch = 0\n\n    for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n        # Step 14: Train\n        train_metrics = train_epoch(model, data, optimizer, loss_fn)\n\n        # Step 15: Validate\n        val_metrics, _, _ = evaluate(model, data, loss_fn, data.val_mask)\n\n        # Step 16: Track metrics\n        for k in ['loss', 'accuracy', 'precision', 'recall', 'f1']:\n            short = {'loss': 'loss', 'accuracy': 'acc', 'precision': 'prec',\n                     'recall': 'rec', 'f1': 'f1'}[k]\n            history[f'train_{short}'].append(train_metrics[k])\n            history[f'val_{short}'].append(val_metrics[k])\n\n        # Step 17: Save best checkpoint based on val F1\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            best_epoch = epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_f1': best_val_f1,\n            }, save_path)\n\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"\\nEpoch {epoch+1:3d} | \"\n                  f\"Train Loss: {train_metrics['loss']:.4f} F1: {train_metrics['f1']:.4f} | \"\n                  f\"Val Loss: {val_metrics['loss']:.4f} F1: {val_metrics['f1']:.4f} \"\n                  f\"Prec: {val_metrics['precision']:.4f} Rec: {val_metrics['recall']:.4f}\")\n\n    print(f\"\\nBest validation F1: {best_val_f1:.4f} at epoch {best_epoch + 1}\")\n    return history, best_val_f1",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2460l416vay",
   "source": "# Initialize model and run training\n\n# Hyperparameters (based on IBM Multi-GNN model_settings.json for GIN)\nMODEL_TYPE = \"gin\"       # \"gin\" or \"gat\"\nN_HIDDEN = 66\nN_GNN_LAYERS = 2\nEDGE_UPDATES = True      # Use edge update MLPs (--emlps flag in original)\nDROPOUT = 0.01\nFINAL_DROPOUT = 0.1\nLR = 0.006\nN_EPOCHS = 100\nSAVE_PATH = \"best_model.pt\"\n\nnum_node_features = data.x.shape[1]\nnum_edge_features = data.edge_attr.shape[1]\n\nif MODEL_TYPE == \"gin\":\n    model = NodeGINe(\n        num_features=num_node_features,\n        num_gnn_layers=N_GNN_LAYERS,\n        n_classes=2,\n        n_hidden=N_HIDDEN,\n        edge_updates=EDGE_UPDATES,\n        edge_dim=num_edge_features,\n        dropout=DROPOUT,\n        final_dropout=FINAL_DROPOUT,\n    )\nelif MODEL_TYPE == \"gat\":\n    model = NodeGATe(\n        num_features=num_node_features,\n        num_gnn_layers=N_GNN_LAYERS,\n        n_classes=2,\n        n_hidden=N_HIDDEN,\n        n_heads=4,\n        edge_updates=EDGE_UPDATES,\n        edge_dim=num_edge_features,\n        dropout=DROPOUT,\n        final_dropout=FINAL_DROPOUT,\n    )\n\nmodel = model.to(device)\ndata = data.to(device)\n\nprint(f\"Model: {MODEL_TYPE.upper()}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(model)\nprint()\n\n# Run training\nhistory, best_val_f1 = train(model, data, n_epochs=N_EPOCHS, lr=LR, save_path=SAVE_PATH)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4k8ptkk79et",
   "source": "## 4. Evaluation\n\nLoad the best checkpoint and run final evaluation on the validation set with confusion matrix and classification report.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f6xbvmtgh1u",
   "source": "# Step 18-19: Load best checkpoint and run final evaluation\n\ncheckpoint = torch.load(SAVE_PATH, map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"Loaded best model from epoch {checkpoint['epoch'] + 1} \"\n      f\"(val F1: {checkpoint['val_f1']:.4f})\\n\")\n\n# Reconstruct loss function for evaluation\nn_sanctioned = (data.y[data.train_mask] == 1).sum().float()\nn_non_sanctioned = (data.y[data.train_mask] == 0).sum().float()\nweight_ratio = (n_non_sanctioned / n_sanctioned).item() if n_sanctioned > 0 else 1.0\nloss_fn = nn.CrossEntropyLoss(\n    weight=torch.FloatTensor([1.0, weight_ratio]).to(device)\n)\n\n# Final validation evaluation\nval_metrics, val_pred, val_logits = evaluate(model, data, loss_fn, data.val_mask)\nval_true = data.y[data.val_mask].cpu().numpy()\nval_pred_np = val_pred.cpu().numpy()\n\nprint(\"=== Final Validation Results ===\")\nfor k, v in val_metrics.items():\n    print(f\"  {k:>10s}: {v:.4f}\")\n\n# Classification report\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(\n    val_true, val_pred_np,\n    target_names=[\"Non-sanctioned\", \"Sanctioned\"],\n    digits=4\n))\n\n# --- Plots ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Confusion matrix heatmap\ncm = confusion_matrix(val_true, val_pred_np)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=[\"Non-sanctioned\", \"Sanctioned\"],\n            yticklabels=[\"Non-sanctioned\", \"Sanctioned\"],\n            ax=axes[0])\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\naxes[0].set_title(\"Confusion Matrix\")\n\n# Training curves\nepochs_range = range(1, len(history['train_f1']) + 1)\naxes[1].plot(epochs_range, history['train_f1'], label='Train F1', alpha=0.8)\naxes[1].plot(epochs_range, history['val_f1'], label='Val F1', alpha=0.8)\naxes[1].plot(epochs_range, history['val_rec'], label='Val Recall', alpha=0.6, linestyle='--')\naxes[1].plot(epochs_range, history['val_prec'], label='Val Precision', alpha=0.6, linestyle='--')\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Score\")\naxes[1].set_title(\"Training Curves\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"training_results.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Plot saved to training_results.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6xr9ty4wecp",
   "source": "# Step 20: Save trained model weights for later use on cross-chain test data\n\nFINAL_SAVE_PATH = \"trained_node_gnn_weights.pt\"\n\ntorch.save({\n    'model_type': MODEL_TYPE,\n    'model_state_dict': model.state_dict(),\n    'num_node_features': num_node_features,\n    'num_edge_features': num_edge_features,\n    'n_hidden': N_HIDDEN,\n    'n_gnn_layers': N_GNN_LAYERS,\n    'edge_updates': EDGE_UPDATES,\n    'n_classes': 2,\n    'dropout': DROPOUT,\n    'final_dropout': FINAL_DROPOUT,\n    'best_val_f1': best_val_f1,\n    'best_epoch': checkpoint['epoch'],\n}, FINAL_SAVE_PATH)\n\nprint(f\"Model weights saved to: {FINAL_SAVE_PATH}\")\nprint()\nprint(\"To reload for cross-chain inference:\")\nprint(\"  ckpt = torch.load('trained_node_gnn_weights.pt')\")\nprint(\"  model = NodeGINe(\")\nprint(\"      num_features=ckpt['num_node_features'],\")\nprint(\"      num_gnn_layers=ckpt['n_gnn_layers'],\")\nprint(\"      n_classes=ckpt['n_classes'],\")\nprint(\"      n_hidden=ckpt['n_hidden'],\")\nprint(\"      edge_updates=ckpt['edge_updates'],\")\nprint(\"      edge_dim=ckpt['num_edge_features'],\")\nprint(\"      dropout=ckpt['dropout'],\")\nprint(\"      final_dropout=ckpt['final_dropout'],\")\nprint(\"  )\")\nprint(\"  model.load_state_dict(ckpt['model_state_dict'])\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}